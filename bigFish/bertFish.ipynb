{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/tmp/ipykernel_11225/3315531538.py:14: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  pattern = '((From .*\\d{4})|(Status: RO))'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3234\n",
      "Return-Path: <james_ngola2002@maktoob.com>\n",
      "X-Sieve: cmu-sieve 2.0\n",
      "Return-Path: <james_ngola2002@maktoob.com>\n",
      "Message-Id: <200210310241.g9V2fNm6028281@cs.CU>\n",
      "From: \"MR. JAMES NGOLA.\" <james_ngola2002@maktoob.com>\n",
      "Reply-To: james_ngola2002@maktoob.com\n",
      "To: webmaster@aclweb.org\n",
      "Date: Thu, 31 Oct 2002 02:38:20 +0000\n",
      "Subject: URGENT BUSINESS ASSISTANCE AND PARTNERSHIP\n",
      "X-Mailer: Microsoft Outlook Express 5.00.2919.6900 DM\n",
      "MIME-Version: 1.0\n",
      "Content-Type: text/plain; charset=\"us-ascii\"\n",
      "Content-Transfer-Encoding: 8bit\n",
      "X-MIME-Autoconverted: from quoted-printable to 8bit by sideshowmel.si.UM id g9V2foW24311\n",
      "Status: O\n",
      "\n",
      "FROM:MR. JAMES NGOLA.\n",
      "CONFIDENTIAL TEL: 233-27-587908.\n",
      "E-MAIL: (james_ngola2002@maktoob.com).\n",
      "\n",
      "URGENT BUSINESS ASSISTANCE AND PARTNERSHIP.\n",
      "\n",
      "\n",
      "DEAR FRIEND,\n",
      "\n",
      "I AM ( DR.) JAMES NGOLA, THE PERSONAL ASSISTANCE TO THE LATE CONGOLESE (PRESIDENT LAURENT KABILA) WHO WAS ASSASSINATED BY HIS BODY GUARD ON 16TH JAN. 2001.\n",
      "\n",
      "\n",
      "THE INCIDENT OCCURRED IN OUR PRESENCE WHILE WE WERE HOLDING MEETING WITH HIS EXCELLENCY OVER THE FINANCIAL RETURNS FROM THE DIAMOND SALES IN THE AREAS CONTROLLED BY (D.R.C.) DEMOCRATIC REPUBLIC OF CONGO FORCES AND THEIR FOREIGN ALLIES ANGOLA AND ZIMBABWE, HAVING RECEIVED THE PREVIOUS DAY (USD$100M) ONE HUNDRED MILLION UNITED STATES DOLLARS, CASH IN THREE DIPLOMATIC BOXES ROUTED THROUGH ZIMBABWE.\n",
      "\n",
      "MY PURPOSE OF WRITING YOU THIS LETTER IS TO SOLICIT FOR YOUR ASSISTANCE AS TO BE A COVER TO THE FUND AND ALSO COLLABORATION IN MOVING THE SAID FUND INTO YOUR BANK ACCOUNT THE SUM OF (USD$25M) TWENTY FIVE MILLION UNITED STATES DOLLARS ONLY, WHICH I DEPOSITED WITH A SECURITY COMPANY IN GHANA, IN A DIPLOMATIC BOX AS GOLDS WORTH (USD$25M) TWENTY FIVE MILLION UNITED STATES DOLLARS ONLY FOR SAFE KEEPING IN A SECURITY VAULT FOR ANY FURTHER INVESTMENT PERHAPS IN YOUR COUNTRY. \n",
      "\n",
      "YOU WERE INTRODUCED TO ME BY A RELIABLE FRIEND OF MINE WHO IS A TRAVELLER,AND ALSO A MEMBER OF CHAMBER OF COMMERCE AS A RELIABLE AND TRUSTWORTHY PERSON WHOM I CAN RELY ON AS FOREIGN PARTNER, EVEN THOUGH THE NATURE OF THE TRANSACTION WAS NOT REVEALED TO HIM FOR SECURITY REASONS.\n",
      "\n",
      "\n",
      "THE (USD$25M) WAS PART OF A PROCEEDS FROM DIAMOND TRADE MEANT FOR THE LATE PRESIDENT LAURENT KABILA WHICH WAS DELIVERED THROUGH ZIMBABWE IN DIPLOMATIC BOXES. THE BOXES WERE KEPT UNDER MY CUSTODY BEFORE THE SAD EVENT THAT TOOK THE LIFE OF (MR. PRESIDENT).THE CONFUSION THAT ENSUED AFTER THE ASSASSINATION AND THE SPORADIC SHOOTING AMONG THE FACTIONS, I HAVE TO RUN AWAY FROM THE COUNTRY FOR MY DEAR LIFE AS I AM NOT A SOLDIER BUT A CIVIL SERVANT I CROSSED RIVER CONGO TO OTHER SIDE OF CONGO LIBREVILLE FROM THERE I MOVED TO THE THIRD COUNTRY GHANA WHERE I AM PRESENTLY TAKING REFUGE. \n",
      "\n",
      "AS A MATTER OF FACT, WHAT I URGENTLY NEEDED FROM YOU IS YOUR ASSISTANCE IN MOVING THIS MONEY INTO YOUR ACCOUNT IN YOUR COUNTRY FOR INVESTMENT WITHOUT RAISING EYEBROW. FOR YOUR ASSISTANCE I WILL GIVE YOU 20% OF THE TOTAL SUM AS YOUR OWN SHARE WHEN THE MONEY GETS TO YOUR ACCOUNT, WHILE 75% WILL BE FOR ME, OF WHICH WITH YOUR KIND ADVICE I HOPE TO INVEST IN PROFITABLE VENTURE IN YOUR COUNTRY IN OTHER TO SETTLE DOWN FOR MEANINGFUL LIFE, AS I AM TIRED OF LIVING IN A WAR ENVIRONMENT. \n",
      "\n",
      "THE REMAINING 5% WILL BE USED TO OFFSET ANY COST INCURRED IN THE CAUSE OF MOVING THE MONEY TO YOUR ACCOUNT. IF THE PROPOSAL IS ACCEPTABLE TO YOU PLEASE CONTACT ME IMMEDIATELY THROUGH THE ABOVE TELEPHONE AND E-MAIL, TO ENABLE ME ARRANGE FACE TO FACE MEETING WITH YOU IN GHANA FOR THE CLEARANCE OF THE FUNDS BEFORE TRANSFRING IT TO YOUR BANK ACCOUNT AS SEEING IS BELIEVING. \n",
      "\n",
      "FINALLY, IT IS IMPORTANT ALSO THAT I LET YOU UNDERSTAND THAT THERE IS NO RISK INVOLVED WHATSOEVER AS THE MONEY HAD NO RECORD IN KINSHASA FOR IT WAS MEANT FOR THE PERSONAL USE OF (MR. PRESIDEND ) BEFORE THE NEFARIOUS INCIDENT OCCURRED, AND ALSO I HAVE ALL THE NECESSARY DOCUMENTS AS REGARDS TO THE FUNDS INCLUDING THE (CERTIFICATE OF DEPOSIT), AS I AM THE DEPOSITOR OF THE CONSIGNMENT.\n",
      "\n",
      "\n",
      "LOOKING FORWARD TO YOUR URGENT RESPONSE.\n",
      "\n",
      "YOUR SINCERELY,\n",
      "\n",
      "MR. JAMES NGOLA. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4000\n",
      "4224\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "%run ./seg_fraud.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/tmp/ipykernel_11225/3718832728.py:14: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  pattern = '((X-FileName: \\w*)|(X-Origin: \\w*))'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "%run ./seg_normal.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./fish_cleaner.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open('fraud.txt', 'r')\n",
    "f2 = open('enron3.txt', 'r')\n",
    "\n",
    "corpus_one = [item for item in f1]\n",
    "corpus_two = [item for item in f2]\n",
    "\n",
    "fs = split_set(corpus_one)\n",
    "ns = split_enron_set(corpus_two)\n",
    "\n",
    "fraud_set = []\n",
    "norm_set = []\n",
    "\n",
    "for i in range(len(fs)):\n",
    "    fraud_set.append(super_clean(fs[i]))\n",
    "for i in range(len(ns)):\n",
    "    norm_set.append(super_clean(ns[i]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_screen(x):        #remove emails from datasets that are less than 512 tokens long. \n",
    "    z = []\n",
    "    for i in range(len(x)):\n",
    "        temp = x[i].split()\n",
    "        if len(temp) >= 512:\n",
    "            z.append(x[i])\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of fraud set before 512 token screening: 3234\n",
      "Length of normal set before 512 token screening: 8630\n",
      "\n",
      "\n",
      "Length of fraud after before 512 token screening: 1608\n",
      "Length of normal after before 512 token screening: 655\n"
     ]
    }
   ],
   "source": [
    "print('Length of fraud set before 512 token screening: {}'.format(len(fraud_set)))\n",
    "print('Length of normal set before 512 token screening: {}'.format(len(norm_set)))\n",
    "\n",
    "fraud_set = bert_screen(fraud_set)\n",
    "norm_set = bert_screen(norm_set)\n",
    "\n",
    "print('\\n')\n",
    "print('Length of fraud after before 512 token screening: {}'.format(len(fraud_set)))\n",
    "print('Length of normal after before 512 token screening: {}'.format(len(norm_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_set = fraud_set[:500]\n",
    "norm_set = norm_set[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(fraud, norm):\n",
    "    new_set = []\n",
    "    for item in fraud:\n",
    "        t = (item, 'fraud')\n",
    "        new_set.append(t)\n",
    "    for item in norm:\n",
    "        t = (item, 'norm')\n",
    "        new_set.append(t)\n",
    "    return new_set    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = label(fraud_set,norm_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "import torch\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0      1\n",
      "0                  FROM:MR. JAMES NGOLA.    URGEN...  fraud\n",
      "1               Dear Friend,  I am Mr. Ben Sulema...  fraud\n",
      "2   FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF E...  fraud\n",
      "3   FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF E...  fraud\n",
      "4   Dear sir,    It is with a heart full of hope ...  fraud\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df = g\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'fraud': 500, 'norm': 500})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(df[1].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={0: \"text\", 1: \"type\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FROM:MR. JAMES NGOLA.    URGEN...</td>\n",
       "      <td>fraud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dear Friend,  I am Mr. Ben Sulema...</td>\n",
       "      <td>fraud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF E...</td>\n",
       "      <td>fraud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF E...</td>\n",
       "      <td>fraud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dear sir,    It is with a heart full of hope ...</td>\n",
       "      <td>fraud</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   type\n",
       "0                  FROM:MR. JAMES NGOLA.    URGEN...  fraud\n",
       "1               Dear Friend,  I am Mr. Ben Sulema...  fraud\n",
       "2   FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF E...  fraud\n",
       "3   FROM HIS ROYAL MAJESTY (HRM) CROWN RULER OF E...  fraud\n",
       "4   Dear sir,    It is with a heart full of hope ...  fraud"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fraud = df[df['type'] == 'fraud'] \n",
    "df_norm = df[df['type'] == 'norm'] \n",
    "df_norm = df_norm.sample(n=len(df_fraud))\n",
    "df = pd.concat([df_norm ,df_fraud], ignore_index=True)\n",
    "df = df.sample(frac=1, random_state = 24).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df.head(800)\n",
    "test_data = df.tail(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array(train_data['text'])\n",
    "x2 = np.array(train_data['type'])\n",
    "\n",
    "y1 = np.array(test_data['text'])\n",
    "y2 = np.array(test_data['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = []\n",
    "t2 = []\n",
    "\n",
    "for i in range(len(x1)):\n",
    "    t1.append({'text': x1[i], 'type': x2[i]})\n",
    "    \n",
    "for i in range(len(y1)):\n",
    "    t2.append({'text': y1[i], 'type': y2[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = t1\n",
    "test_data = t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['type']), train_data)))\n",
    "test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['type']), test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 231508/231508 [00:00<00:00, 826924.92B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], train_texts))\n",
    "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], test_texts))\n",
    "train_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, train_tokens))\n",
    "test_tokens_ids = list(map(tokenizer.convert_tokens_to_ids, test_tokens))\n",
    "train_tokens_ids = pad_sequences(train_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "test_tokens_ids = pad_sequences(test_tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell above tokenizes text sequences of up to 512 tokens, as that is the max input size of BERT.\n",
    "Datapoints posessing less than 512 tokens are \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = []\n",
    "test_y = []\n",
    "t1 = []\n",
    "t2 = []\n",
    "\n",
    "for item in train_labels:\n",
    "    if str(item) == 'fraud': t1.append(1)\n",
    "    else: t1.append(0)    \n",
    "\n",
    "        \n",
    "for item in test_labels:\n",
    "    if str(item) == 'fraud': t2.append(1)\n",
    "    else: t2.append(0)        \n",
    "    \n",
    "train_y = np.array(t1)\n",
    "test_y = np.array(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DAtA LABEL VECTOR\n",
      "[0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 1\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1\n",
      " 0 1 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0\n",
      " 1 1 0 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 1 1 0\n",
      " 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1\n",
      " 0 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0\n",
      " 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1\n",
      " 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 1\n",
      " 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 0 0 1\n",
      " 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 1\n",
      " 1 0 0 1 1 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1\n",
      " 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 1 0 1\n",
      " 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1\n",
      " 1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0\n",
      " 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 1 0 1 1 0\n",
      " 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0\n",
      " 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1\n",
      " 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1\n",
      " 1 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1\n",
      " 1 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
      " 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0]\n",
      "[1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0 1\n",
      " 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 1 0\n",
      " 1 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING DAtA LABEL VECTOR\")\n",
    "print(train_y)\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertBinaryClassifier, self).__init__() #DEFAULT CONSTRUCTOR INIT\n",
    "    \n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased') #USE BERT BASE FOR LIGHTER LOAD\n",
    "        self.dropout = nn.Dropout(dropout) #DROPOUT = DEFAULT\n",
    "        self.linear = nn.Linear(768, 1)    #LINEAR ACTIVATION LAYER, HIDDEN VECTOR LENGTH 768\n",
    "        self.sigmoid = nn.Sigmoid()        #SIGMOID ACTIVATION LAYER, S SHAPE DECISION BOUNDARY\n",
    "    \n",
    "    def forward(self, tokens, masks=None):\n",
    "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba\n",
    "    #FUNCTION ABOVE DEFINES FLOW OF DATA FOR BERT MODEL.\n",
    "    #POOLED OUTPUT = DEFAULT BERT POOLED OUTPUT, ABSTRACTION OF DATAPOINT\n",
    "    #DROPOUT = DEFAULT BERT DROPOUT\n",
    "    #LINEAR_OUTPUT = LINEAR ACTIVATION WITH HIDDEN SIZE 768\n",
    "    #FINAL STEP, PUT RESULT INTO SIGMOID ACTIVATION FOR RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
    "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]\n",
    "train_masks_tensor = torch.tensor(train_masks)\n",
    "test_masks_tensor = torch.tensor(test_masks)\n",
    "\n",
    "#CREATE TENSORS FOR TRAINING. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
    "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
    "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
    "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset =  torch.utils.data.TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
    "train_sampler =  torch.utils.data.RandomSampler(train_dataset)\n",
    "train_dataloader =  torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=1)\n",
    "test_dataset =  torch.utils.data.TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
    "test_sampler =  torch.utils.data.SequentialSampler(test_dataset)\n",
    "test_dataloader =  torch.utils.data.DataLoader(test_dataset, sampler=test_sampler, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STATEMENT BELOW ITERATES THROUGH EACH ITEM IN THE TRAINING DATASET AT ONE EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|███▏                       | 48358400/407873900 [02:22<21:42, 276081.00B/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      2\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 3\u001b[0m bert_clf \u001b[38;5;241m=\u001b[39m \u001b[43mBertBinaryClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(bert_clf\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-6\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n",
      "Cell \u001b[0;32mIn[54], line 5\u001b[0m, in \u001b[0;36mBertBinaryClassifier.__init__\u001b[0;34m(self, dropout)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m(BertBinaryClassifier, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m() \u001b[38;5;66;03m#DEFAULT CONSTRUCTOR INIT\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert \u001b[38;5;241m=\u001b[39m \u001b[43mBertModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#USE BERT BASE FOR LIGHTER LOAD\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(dropout) \u001b[38;5;66;03m#DROPOUT = DEFAULT\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m768\u001b[39m, \u001b[38;5;241m1\u001b[39m)    \u001b[38;5;66;03m#LINEAR ACTIVATION LAYER, HIDDEN VECTOR LENGTH 768\u001b[39;00m\n",
      "File \u001b[0;32m~/bert_env/lib64/python3.12/site-packages/pytorch_pretrained_bert/modeling.py:566\u001b[0m, in \u001b[0;36mBertPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# redirect to the cache, if necessary\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 566\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchive_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\n\u001b[1;32m    569\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m was not found in model name list (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe assumed \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m was a path or url but couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any file \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(PRETRAINED_MODEL_ARCHIVE_MAP\u001b[38;5;241m.\u001b[39mkeys()),\n\u001b[1;32m    574\u001b[0m             archive_file))\n",
      "File \u001b[0;32m~/bert_env/lib64/python3.12/site-packages/pytorch_pretrained_bert/file_utils.py:106\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir)\u001b[0m\n\u001b[1;32m    102\u001b[0m parsed \u001b[38;5;241m=\u001b[39m urlparse(url_or_filename)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parsed\u001b[38;5;241m.\u001b[39mscheme \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m url_or_filename\n",
      "File \u001b[0;32m~/bert_env/lib64/python3.12/site-packages/pytorch_pretrained_bert/file_utils.py:230\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir)\u001b[0m\n\u001b[1;32m    228\u001b[0m     s3_get(url, temp_file)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# we are copying the file before closing it, so flush to avoid truncation\u001b[39;00m\n\u001b[1;32m    233\u001b[0m temp_file\u001b[38;5;241m.\u001b[39mflush()\n",
      "File \u001b[0;32m~/bert_env/lib64/python3.12/site-packages/pytorch_pretrained_bert/file_utils.py:172\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file)\u001b[0m\n\u001b[1;32m    170\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(content_length) \u001b[38;5;28;01mif\u001b[39;00m content_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    171\u001b[0m progress \u001b[38;5;241m=\u001b[39m tqdm(unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39mtotal)\n\u001b[0;32m--> 172\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bert_env/lib64/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/bert_env/lib64/python3.12/site-packages/urllib3/response.py:1043\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1043\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1046\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/bert_env/lib64/python3.12/site-packages/urllib3/response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 935\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/bert_env/lib64/python3.12/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 862\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/bert_env/lib64/python3.12/site-packages/urllib3/response.py:845\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/lib64/python3.12/http/client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib64/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.12/ssl.py:1253\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1252\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib64/python3.12/ssl.py:1105\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "359/800.0 loss: 0.37012427219500144 \n",
      "Epoch:  1\n",
      "360/800.0 loss: 0.36945133268255276 \n",
      "Epoch:  1\n",
      "361/800.0 loss: 0.36868361062601784 \n",
      "Epoch:  1\n",
      "362/800.0 loss: 0.3678928053740299 \n",
      "Epoch:  1\n",
      "363/800.0 loss: 0.367072213359259 \n",
      "Epoch:  1\n",
      "364/800.0 loss: 0.36627642073451655 \n",
      "Epoch:  1\n",
      "365/800.0 loss: 0.36561136816938716 \n",
      "Epoch:  1\n",
      "366/800.0 loss: 0.36489207405160495 \n",
      "Epoch:  1\n",
      "367/800.0 loss: 0.3642006463209248 \n",
      "Epoch:  1\n",
      "368/800.0 loss: 0.3634364438775755 \n",
      "Epoch:  1\n",
      "369/800.0 loss: 0.3626897105695428 \n",
      "Epoch:  1\n",
      "370/800.0 loss: 0.3620248606825132 \n",
      "Epoch:  1\n",
      "371/800.0 loss: 0.36133277354141075 \n",
      "Epoch:  1\n",
      "372/800.0 loss: 0.3605846480613739 \n",
      "Epoch:  1\n",
      "373/800.0 loss: 0.3598412141243723 \n",
      "Epoch:  1\n",
      "374/800.0 loss: 0.35906537433465324 \n",
      "Epoch:  1\n",
      "375/800.0 loss: 0.3583896152001429 \n",
      "Epoch:  1\n",
      "376/800.0 loss: 0.35770695020175425 \n",
      "Epoch:  1\n",
      "377/800.0 loss: 0.35696319910505464 \n",
      "Epoch:  1\n",
      "378/800.0 loss: 0.35620460181563385 \n",
      "Epoch:  1\n",
      "379/800.0 loss: 0.3555578691982909 \n",
      "Epoch:  1\n",
      "380/800.0 loss: 0.35491609248745787 \n",
      "Epoch:  1\n",
      "381/800.0 loss: 0.3542038740795008 \n",
      "Epoch:  1\n",
      "382/800.0 loss: 0.35363870588746454 \n",
      "Epoch:  1\n",
      "383/800.0 loss: 0.35290749155683443 \n",
      "Epoch:  1\n",
      "384/800.0 loss: 0.3521379567392461 \n",
      "Epoch:  1\n",
      "385/800.0 loss: 0.3515010612770683 \n",
      "Epoch:  1\n",
      "386/800.0 loss: 0.3508598011477973 \n",
      "Epoch:  1\n",
      "387/800.0 loss: 0.35016129741963653 \n",
      "Epoch:  1\n",
      "388/800.0 loss: 0.349509116846369 \n",
      "Epoch:  1\n",
      "389/800.0 loss: 0.3488753868983342 \n",
      "Epoch:  1\n",
      "390/800.0 loss: 0.3482941636039168 \n",
      "Epoch:  1\n",
      "391/800.0 loss: 0.3476512028588628 \n",
      "Epoch:  1\n",
      "392/800.0 loss: 0.34693722246788233 \n",
      "Epoch:  1\n",
      "393/800.0 loss: 0.3463564046899679 \n",
      "Epoch:  1\n",
      "394/800.0 loss: 0.34576064825435227 \n",
      "Epoch:  1\n",
      "395/800.0 loss: 0.3453610247621934 \n",
      "Epoch:  1\n",
      "396/800.0 loss: 0.34474447255095547 \n",
      "Epoch:  1\n",
      "397/800.0 loss: 0.3441178170617801 \n",
      "Epoch:  1\n",
      "398/800.0 loss: 0.3434337358688352 \n",
      "Epoch:  1\n",
      "399/800.0 loss: 0.3427884056977928 \n",
      "Epoch:  1\n",
      "400/800.0 loss: 0.34217005865903866 \n",
      "Epoch:  1\n",
      "401/800.0 loss: 0.3414483858893315 \n",
      "Epoch:  1\n",
      "402/800.0 loss: 0.34080804865021563 \n",
      "Epoch:  1\n",
      "403/800.0 loss: 0.3401651043389546 \n",
      "Epoch:  1\n",
      "404/800.0 loss: 0.33950258965293567 \n",
      "Epoch:  1\n",
      "405/800.0 loss: 0.3388695127326133 \n",
      "Epoch:  1\n",
      "406/800.0 loss: 0.33822899351139035 \n",
      "Epoch:  1\n",
      "407/800.0 loss: 0.3376499468223283 \n",
      "Epoch:  1\n",
      "408/800.0 loss: 0.33697742900485517 \n",
      "Epoch:  1\n",
      "409/800.0 loss: 0.33631880802170533 \n",
      "Epoch:  1\n",
      "410/800.0 loss: 0.33564835640889595 \n",
      "Epoch:  1\n",
      "411/800.0 loss: 0.3350372919089441 \n",
      "Epoch:  1\n",
      "412/800.0 loss: 0.3343968326744531 \n",
      "Epoch:  1\n",
      "413/800.0 loss: 0.3337724489668285 \n",
      "Epoch:  1\n",
      "414/800.0 loss: 0.33315399182309585 \n",
      "Epoch:  1\n",
      "415/800.0 loss: 0.3326438698075855 \n",
      "Epoch:  1\n",
      "416/800.0 loss: 0.33207150501348703 \n",
      "Epoch:  1\n",
      "417/800.0 loss: 0.33145108541839147 \n",
      "Epoch:  1\n",
      "418/800.0 loss: 0.3308530527129236 \n",
      "Epoch:  1\n",
      "419/800.0 loss: 0.3302374265644522 \n",
      "Epoch:  1\n",
      "420/800.0 loss: 0.32962659603944866 \n",
      "Epoch:  1\n",
      "421/800.0 loss: 0.32904636601242127 \n",
      "Epoch:  1\n",
      "422/800.0 loss: 0.3284257600792897 \n",
      "Epoch:  1\n",
      "423/800.0 loss: 0.32786049204439205 \n",
      "Epoch:  1\n",
      "424/800.0 loss: 0.3273013687922674 \n",
      "Epoch:  1\n",
      "425/800.0 loss: 0.32683519939476613 \n",
      "Epoch:  1\n",
      "426/800.0 loss: 0.3262547879486369 \n",
      "Epoch:  1\n",
      "427/800.0 loss: 0.3256070590621539 \n",
      "Epoch:  1\n",
      "428/800.0 loss: 0.3250401720039911 \n",
      "Epoch:  1\n",
      "429/800.0 loss: 0.32447766007552314 \n",
      "Epoch:  1\n",
      "430/800.0 loss: 0.3239139933441604 \n",
      "Epoch:  1\n",
      "431/800.0 loss: 0.3233688180938501 \n",
      "Epoch:  1\n",
      "432/800.0 loss: 0.3227820469122423 \n",
      "Epoch:  1\n",
      "433/800.0 loss: 0.3222111613674235 \n",
      "Epoch:  1\n",
      "434/800.0 loss: 0.32161536039463406 \n",
      "Epoch:  1\n",
      "435/800.0 loss: 0.32113618763385837 \n",
      "Epoch:  1\n",
      "436/800.0 loss: 0.3205125463073668 \n",
      "Epoch:  1\n",
      "437/800.0 loss: 0.3199194970808617 \n",
      "Epoch:  1\n",
      "438/800.0 loss: 0.31937298081934046 \n",
      "Epoch:  1\n",
      "439/800.0 loss: 0.31882989939979534 \n",
      "Epoch:  1\n",
      "440/800.0 loss: 0.3182384910615258 \n",
      "Epoch:  1\n",
      "441/800.0 loss: 0.3177214289564488 \n",
      "Epoch:  1\n",
      "442/800.0 loss: 0.3171448460433203 \n",
      "Epoch:  1\n",
      "443/800.0 loss: 0.31663857683170216 \n",
      "Epoch:  1\n",
      "444/800.0 loss: 0.3160819760450486 \n",
      "Epoch:  1\n",
      "445/800.0 loss: 0.3155045391649275 \n",
      "Epoch:  1\n",
      "446/800.0 loss: 0.31495406336132314 \n",
      "Epoch:  1\n",
      "447/800.0 loss: 0.3144160218653269 \n",
      "Epoch:  1\n",
      "448/800.0 loss: 0.31386735447099584 \n",
      "Epoch:  1\n",
      "449/800.0 loss: 0.3132839046915372 \n",
      "Epoch:  1\n",
      "450/800.0 loss: 0.3126991968560642 \n",
      "Epoch:  1\n",
      "451/800.0 loss: 0.3121246856480705 \n",
      "Epoch:  1\n",
      "452/800.0 loss: 0.31155069854941064 \n",
      "Epoch:  1\n",
      "453/800.0 loss: 0.31097819962198253 \n",
      "Epoch:  1\n",
      "454/800.0 loss: 0.3104695164649696 \n",
      "Epoch:  1\n",
      "455/800.0 loss: 0.3099112292147127 \n",
      "Epoch:  1\n",
      "456/800.0 loss: 0.30935782755784186 \n",
      "Epoch:  1\n",
      "457/800.0 loss: 0.308812427899788 \n",
      "Epoch:  1\n",
      "458/800.0 loss: 0.30823096690581775 \n",
      "Epoch:  1\n",
      "459/800.0 loss: 0.30769709302517384 \n",
      "Epoch:  1\n",
      "460/800.0 loss: 0.3071843296564194 \n",
      "Epoch:  1\n",
      "461/800.0 loss: 0.3121428934865313 \n",
      "Epoch:  1\n",
      "462/800.0 loss: 0.3116333476362295 \n",
      "Epoch:  1\n",
      "463/800.0 loss: 0.31111320169995826 \n",
      "Epoch:  1\n",
      "464/800.0 loss: 0.3105612767880322 \n",
      "Epoch:  1\n",
      "465/800.0 loss: 0.3100077065643195 \n",
      "Epoch:  1\n",
      "466/800.0 loss: 0.3094608517048573 \n",
      "Epoch:  1\n",
      "467/800.0 loss: 0.30895753831276274 \n",
      "Epoch:  1\n",
      "468/800.0 loss: 0.30842404697400166 \n",
      "Epoch:  1\n",
      "469/800.0 loss: 0.3078646117781705 \n",
      "Epoch:  1\n",
      "470/800.0 loss: 0.307316001931763 \n",
      "Epoch:  1\n",
      "471/800.0 loss: 0.30675660362625023 \n",
      "Epoch:  1\n",
      "472/800.0 loss: 0.306201980856836 \n",
      "Epoch:  1\n",
      "473/800.0 loss: 0.305725884981422 \n",
      "Epoch:  1\n",
      "474/800.0 loss: 0.30518392230335034 \n",
      "Epoch:  1\n",
      "475/800.0 loss: 0.3047352185582413 \n",
      "Epoch:  1\n",
      "476/800.0 loss: 0.30420887234388405 \n",
      "Epoch:  1\n",
      "477/800.0 loss: 0.3037764028057643 \n",
      "Epoch:  1\n",
      "478/800.0 loss: 0.3032602333932306 \n",
      "Epoch:  1\n",
      "479/800.0 loss: 0.3027951986140882 \n",
      "Epoch:  1\n",
      "480/800.0 loss: 0.30232893276704076 \n",
      "Epoch:  1\n",
      "481/800.0 loss: 0.30181620317932484 \n",
      "Epoch:  1\n",
      "482/800.0 loss: 0.3013352886860415 \n",
      "Epoch:  1\n",
      "483/800.0 loss: 0.3008148623861311 \n",
      "Epoch:  1\n",
      "484/800.0 loss: 0.3004036509038247 \n",
      "Epoch:  1\n",
      "485/800.0 loss: 0.29989035922742674 \n",
      "Epoch:  1\n",
      "486/800.0 loss: 0.2993657312660002 \n",
      "Epoch:  1\n",
      "487/800.0 loss: 0.2988550697270109 \n",
      "Epoch:  1\n",
      "488/800.0 loss: 0.29857742552529326 \n",
      "Epoch:  1\n",
      "489/800.0 loss: 0.2982027747573293 \n",
      "Epoch:  1\n",
      "490/800.0 loss: 0.29778728880701383 \n",
      "Epoch:  1\n",
      "491/800.0 loss: 0.2972849429077734 \n",
      "Epoch:  1\n",
      "492/800.0 loss: 0.29688757274141175 \n",
      "Epoch:  1\n",
      "493/800.0 loss: 0.2963865289336395 \n",
      "Epoch:  1\n",
      "494/800.0 loss: 0.29586933961119316 \n",
      "Epoch:  1\n",
      "495/800.0 loss: 0.29546541857322856 \n",
      "Epoch:  1\n",
      "496/800.0 loss: 0.29497310506476004 \n",
      "Epoch:  1\n",
      "497/800.0 loss: 0.29448222077216013 \n",
      "Epoch:  1\n",
      "498/800.0 loss: 0.29400539735634484 \n",
      "Epoch:  1\n",
      "499/800.0 loss: 0.2935607428997755 \n",
      "Epoch:  1\n",
      "500/800.0 loss: 0.29309167849000345 \n",
      "Epoch:  1\n",
      "501/800.0 loss: 0.2925920755160876 \n",
      "Epoch:  1\n",
      "502/800.0 loss: 0.2921456089298839 \n",
      "Epoch:  1\n",
      "503/800.0 loss: 0.29168304956207675 \n",
      "Epoch:  1\n",
      "504/800.0 loss: 0.29119048790618923 \n",
      "Epoch:  1\n",
      "505/800.0 loss: 0.2906990471137724 \n",
      "Epoch:  1\n",
      "506/800.0 loss: 0.29025220833498344 \n",
      "Epoch:  1\n",
      "507/800.0 loss: 0.28980791370025066 \n",
      "Epoch:  1\n",
      "508/800.0 loss: 0.2893590703533417 \n",
      "Epoch:  1\n",
      "509/800.0 loss: 0.2889272100566065 \n",
      "Epoch:  1\n",
      "510/800.0 loss: 0.2884428760441432 \n",
      "Epoch:  1\n",
      "511/800.0 loss: 0.2879709802582511 \n",
      "Epoch:  1\n",
      "512/800.0 loss: 0.2874904822913992 \n",
      "Epoch:  1\n",
      "513/800.0 loss: 0.2902794492644791 \n",
      "Epoch:  1\n",
      "514/800.0 loss: 0.289812849331828 \n",
      "Epoch:  1\n",
      "515/800.0 loss: 0.2893285692286815 \n",
      "Epoch:  1\n",
      "516/800.0 loss: 0.2888796617289576 \n",
      "Epoch:  1\n",
      "517/800.0 loss: 0.28842531870978677 \n",
      "Epoch:  1\n",
      "518/800.0 loss: 0.28801789545312323 \n",
      "Epoch:  1\n",
      "519/800.0 loss: 0.28759533148258926 \n",
      "Epoch:  1\n",
      "520/800.0 loss: 0.2871326323591473 \n",
      "Epoch:  1\n",
      "521/800.0 loss: 0.2869511187519036 \n",
      "Epoch:  1\n",
      "522/800.0 loss: 0.28673269472175755 \n",
      "Epoch:  1\n",
      "523/800.0 loss: 0.2867110877134304 \n",
      "Epoch:  1\n",
      "524/800.0 loss: 0.28897142392538844 \n",
      "Epoch:  1\n",
      "525/800.0 loss: 0.2885104704917384 \n",
      "Epoch:  1\n",
      "526/800.0 loss: 0.2881181998570685 \n",
      "Epoch:  1\n",
      "527/800.0 loss: 0.2876626073848456 \n",
      "Epoch:  1\n",
      "528/800.0 loss: 0.28720856155841024 \n",
      "Epoch:  1\n",
      "529/800.0 loss: 0.28681580180407695 \n",
      "Epoch:  1\n",
      "530/800.0 loss: 0.2863574492097911 \n",
      "Epoch:  1\n",
      "531/800.0 loss: 0.28595849324746014 \n",
      "Epoch:  1\n",
      "532/800.0 loss: 0.2854887967890803 \n",
      "Epoch:  1\n",
      "533/800.0 loss: 0.28507909039907464 \n",
      "Epoch:  1\n",
      "534/800.0 loss: 0.2847701481082172 \n",
      "Epoch:  1\n",
      "535/800.0 loss: 0.28434725426165247 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "536/800.0 loss: 0.2839167351163299 \n",
      "Epoch:  1\n",
      "537/800.0 loss: 0.28353934203271086 \n",
      "Epoch:  1\n",
      "538/800.0 loss: 0.28310098727829824 \n",
      "Epoch:  1\n",
      "539/800.0 loss: 0.2826654157221869 \n",
      "Epoch:  1\n",
      "540/800.0 loss: 0.28223857824826637 \n",
      "Epoch:  1\n",
      "541/800.0 loss: 0.2818134607080494 \n",
      "Epoch:  1\n",
      "542/800.0 loss: 0.281420288953937 \n",
      "Epoch:  1\n",
      "543/800.0 loss: 0.2810260648770696 \n",
      "Epoch:  1\n",
      "544/800.0 loss: 0.280630404720886 \n",
      "Epoch:  1\n",
      "545/800.0 loss: 0.28021624732776224 \n",
      "Epoch:  1\n",
      "546/800.0 loss: 0.2798180010108451 \n",
      "Epoch:  1\n",
      "547/800.0 loss: 0.279422997531012 \n",
      "Epoch:  1\n",
      "548/800.0 loss: 0.27907088792497775 \n",
      "Epoch:  1\n",
      "549/800.0 loss: 0.27866587459363723 \n",
      "Epoch:  1\n",
      "550/800.0 loss: 0.27831451613084157 \n",
      "Epoch:  1\n",
      "551/800.0 loss: 0.2779061832006319 \n",
      "Epoch:  1\n",
      "552/800.0 loss: 0.2775710156056053 \n",
      "Epoch:  1\n",
      "553/800.0 loss: 0.2771576190105091 \n",
      "Epoch:  1\n",
      "554/800.0 loss: 0.2767575132551494 \n",
      "Epoch:  1\n",
      "555/800.0 loss: 0.2763274118987669 \n",
      "Epoch:  1\n",
      "556/800.0 loss: 0.27592311439704637 \n",
      "Epoch:  1\n",
      "557/800.0 loss: 0.2755234407988714 \n",
      "Epoch:  1\n",
      "558/800.0 loss: 0.2750879436038261 \n",
      "Epoch:  1\n",
      "559/800.0 loss: 0.27471254557105046 \n",
      "Epoch:  1\n",
      "560/800.0 loss: 0.2742928463480902 \n",
      "Epoch:  1\n",
      "561/800.0 loss: 0.2739163407545497 \n",
      "Epoch:  1\n",
      "562/800.0 loss: 0.27349724052854374 \n",
      "Epoch:  1\n",
      "563/800.0 loss: 0.27309060997679724 \n",
      "Epoch:  1\n",
      "564/800.0 loss: 0.27268252395159376 \n",
      "Epoch:  1\n",
      "565/800.0 loss: 0.2723384890244622 \n",
      "Epoch:  1\n",
      "566/800.0 loss: 0.2719239923491995 \n",
      "Epoch:  1\n",
      "567/800.0 loss: 0.2715108789188761 \n",
      "Epoch:  1\n",
      "568/800.0 loss: 0.2710955946563836 \n",
      "Epoch:  1\n",
      "569/800.0 loss: 0.2707088718223467 \n",
      "Epoch:  1\n",
      "570/800.0 loss: 0.27037364858383683 \n",
      "Epoch:  1\n",
      "571/800.0 loss: 0.2699878334712524 \n",
      "Epoch:  1\n",
      "572/800.0 loss: 0.26962264408121767 \n",
      "Epoch:  1\n",
      "573/800.0 loss: 0.26923323801689447 \n",
      "Epoch:  1\n",
      "574/800.0 loss: 0.2688402244513449 \n",
      "Epoch:  1\n",
      "575/800.0 loss: 0.2684552622247591 \n",
      "Epoch:  1\n",
      "576/800.0 loss: 0.2681007832998936 \n",
      "Epoch:  1\n",
      "577/800.0 loss: 0.2677541295820557 \n",
      "Epoch:  1\n",
      "578/800.0 loss: 0.26737499226766137 \n",
      "Epoch:  1\n",
      "579/800.0 loss: 0.2669649056319533 \n",
      "Epoch:  1\n",
      "580/800.0 loss: 0.26657000278313475 \n",
      "Epoch:  1\n",
      "581/800.0 loss: 0.2662050008223331 \n",
      "Epoch:  1\n",
      "582/800.0 loss: 0.26579651902677365 \n",
      "Epoch:  1\n",
      "583/800.0 loss: 0.26540197397276044 \n",
      "Epoch:  1\n",
      "584/800.0 loss: 0.26504596716636775 \n",
      "Epoch:  1\n",
      "585/800.0 loss: 0.2647066606501111 \n",
      "Epoch:  1\n",
      "586/800.0 loss: 0.2643518541833256 \n",
      "Epoch:  1\n",
      "587/800.0 loss: 0.264028694635878 \n",
      "Epoch:  1\n",
      "588/800.0 loss: 0.26366375335800524 \n",
      "Epoch:  1\n",
      "589/800.0 loss: 0.26328275121306466 \n",
      "Epoch:  1\n",
      "590/800.0 loss: 0.2628962728848867 \n",
      "Epoch:  1\n",
      "591/800.0 loss: 0.2625216769614584 \n",
      "Epoch:  1\n",
      "592/800.0 loss: 0.26213406511028037 \n",
      "Epoch:  1\n",
      "593/800.0 loss: 0.2617679141455578 \n",
      "Epoch:  1\n",
      "594/800.0 loss: 0.26138015169055523 \n",
      "Epoch:  1\n",
      "595/800.0 loss: 0.2610123162314096 \n",
      "Epoch:  1\n",
      "596/800.0 loss: 0.26064516672586874 \n",
      "Epoch:  1\n",
      "597/800.0 loss: 0.26028973892032103 \n",
      "Epoch:  1\n",
      "598/800.0 loss: 0.25996296356536314 \n",
      "Epoch:  1\n",
      "599/800.0 loss: 0.2595766010383765 \n",
      "Epoch:  1\n",
      "600/800.0 loss: 0.25921402011878675 \n",
      "Epoch:  1\n",
      "601/800.0 loss: 0.25889891439869356 \n",
      "Epoch:  1\n",
      "602/800.0 loss: 0.2585129632983684 \n",
      "Epoch:  1\n",
      "603/800.0 loss: 0.2581705466400067 \n",
      "Epoch:  1\n",
      "604/800.0 loss: 0.2577978186709575 \n",
      "Epoch:  1\n",
      "605/800.0 loss: 0.25741954961416647 \n",
      "Epoch:  1\n",
      "606/800.0 loss: 0.257034228031433 \n",
      "Epoch:  1\n",
      "607/800.0 loss: 0.25668133373102664 \n",
      "Epoch:  1\n",
      "608/800.0 loss: 0.2563055994047222 \n",
      "Epoch:  1\n",
      "609/800.0 loss: 0.2559681941678778 \n",
      "Epoch:  1\n",
      "610/800.0 loss: 0.2556067031858498 \n",
      "Epoch:  1\n",
      "611/800.0 loss: 0.2552374166229533 \n",
      "Epoch:  1\n",
      "612/800.0 loss: 0.25490590287934606 \n",
      "Epoch:  1\n",
      "613/800.0 loss: 0.2545401681311647 \n",
      "Epoch:  1\n",
      "614/800.0 loss: 0.2541897678036031 \n",
      "Epoch:  1\n",
      "615/800.0 loss: 0.2538257743528282 \n",
      "Epoch:  1\n",
      "616/800.0 loss: 0.25345820165880784 \n",
      "Epoch:  1\n",
      "617/800.0 loss: 0.2531106796109609 \n",
      "Epoch:  1\n",
      "618/800.0 loss: 0.2527481707923343 \n",
      "Epoch:  1\n",
      "619/800.0 loss: 0.2524132891287727 \n",
      "Epoch:  1\n",
      "620/800.0 loss: 0.25205983559866457 \n",
      "Epoch:  1\n",
      "621/800.0 loss: 0.2517366090975103 \n",
      "Epoch:  1\n",
      "622/800.0 loss: 0.251385569135938 \n",
      "Epoch:  1\n",
      "623/800.0 loss: 0.2510303448802099 \n",
      "Epoch:  1\n",
      "624/800.0 loss: 0.25069348801374436 \n",
      "Epoch:  1\n",
      "625/800.0 loss: 0.25036305016555344 \n",
      "Epoch:  1\n",
      "626/800.0 loss: 0.25003335847143543 \n",
      "Epoch:  1\n",
      "627/800.0 loss: 0.24968990618303702 \n",
      "Epoch:  1\n",
      "628/800.0 loss: 0.2493464442026255 \n",
      "Epoch:  1\n",
      "629/800.0 loss: 0.24899797113168806 \n",
      "Epoch:  1\n",
      "630/800.0 loss: 0.24867188104571517 \n",
      "Epoch:  1\n",
      "631/800.0 loss: 0.24835112346100469 \n",
      "Epoch:  1\n",
      "632/800.0 loss: 0.24800694567308795 \n",
      "Epoch:  1\n",
      "633/800.0 loss: 0.24771100202757476 \n",
      "Epoch:  1\n",
      "634/800.0 loss: 0.24737101156176544 \n",
      "Epoch:  1\n",
      "635/800.0 loss: 0.24703831375582413 \n",
      "Epoch:  1\n",
      "636/800.0 loss: 0.24670154401912606 \n",
      "Epoch:  1\n",
      "637/800.0 loss: 0.24636183688154323 \n",
      "Epoch:  1\n",
      "638/800.0 loss: 0.24605908536050522 \n",
      "Epoch:  1\n",
      "639/800.0 loss: 0.24571585493104067 \n",
      "Epoch:  1\n",
      "640/800.0 loss: 0.2453700983998556 \n",
      "Epoch:  1\n",
      "641/800.0 loss: 0.24505838766879753 \n",
      "Epoch:  1\n",
      "642/800.0 loss: 0.2447581547623845 \n",
      "Epoch:  1\n",
      "643/800.0 loss: 0.24442460119539167 \n",
      "Epoch:  1\n",
      "644/800.0 loss: 0.2440867493463348 \n",
      "Epoch:  1\n",
      "645/800.0 loss: 0.24375326067624448 \n",
      "Epoch:  1\n",
      "646/800.0 loss: 0.24342723390003998 \n",
      "Epoch:  1\n",
      "647/800.0 loss: 0.24311169430061622 \n",
      "Epoch:  1\n",
      "648/800.0 loss: 0.2428276168386264 \n",
      "Epoch:  1\n",
      "649/800.0 loss: 0.24249912303800766 \n",
      "Epoch:  1\n",
      "650/800.0 loss: 0.24217410455708221 \n",
      "Epoch:  1\n",
      "651/800.0 loss: 0.24190049426790122 \n",
      "Epoch:  1\n",
      "652/800.0 loss: 0.24160679071787944 \n",
      "Epoch:  1\n",
      "653/800.0 loss: 0.24127804990060378 \n",
      "Epoch:  1\n",
      "654/800.0 loss: 0.24096606554363975 \n",
      "Epoch:  1\n",
      "655/800.0 loss: 0.2406753117712669 \n",
      "Epoch:  1\n",
      "656/800.0 loss: 0.2403509503023492 \n",
      "Epoch:  1\n",
      "657/800.0 loss: 0.24004319262914472 \n",
      "Epoch:  1\n",
      "658/800.0 loss: 0.23974450066832653 \n",
      "Epoch:  1\n",
      "659/800.0 loss: 0.23942477436679782 \n",
      "Epoch:  1\n",
      "660/800.0 loss: 0.2391011900833488 \n",
      "Epoch:  1\n",
      "661/800.0 loss: 0.23880126866655496 \n",
      "Epoch:  1\n",
      "662/800.0 loss: 0.23847610719479498 \n",
      "Epoch:  1\n",
      "663/800.0 loss: 0.23815635411371758 \n",
      "Epoch:  1\n",
      "664/800.0 loss: 0.2378738012949103 \n",
      "Epoch:  1\n",
      "665/800.0 loss: 0.2375606808968045 \n",
      "Epoch:  1\n",
      "666/800.0 loss: 0.23725935614024146 \n",
      "Epoch:  1\n",
      "667/800.0 loss: 0.2369457733856436 \n",
      "Epoch:  1\n",
      "668/800.0 loss: 0.2366298768023427 \n",
      "Epoch:  1\n",
      "669/800.0 loss: 0.2363220034361775 \n",
      "Epoch:  1\n",
      "670/800.0 loss: 0.23601339813793737 \n",
      "Epoch:  1\n",
      "671/800.0 loss: 0.23569929862527975 \n",
      "Epoch:  1\n",
      "672/800.0 loss: 0.23541504221609938 \n",
      "Epoch:  1\n",
      "673/800.0 loss: 0.23510568388337022 \n",
      "Epoch:  1\n",
      "674/800.0 loss: 0.23479254487212056 \n",
      "Epoch:  1\n",
      "675/800.0 loss: 0.2344979647155741 \n",
      "Epoch:  1\n",
      "676/800.0 loss: 0.2342010772095077 \n",
      "Epoch:  1\n",
      "677/800.0 loss: 0.2338962740317031 \n",
      "Epoch:  1\n",
      "678/800.0 loss: 0.23358967157961344 \n",
      "Epoch:  1\n",
      "679/800.0 loss: 0.23328170461361022 \n",
      "Epoch:  1\n",
      "680/800.0 loss: 0.23299211524044014 \n",
      "Epoch:  1\n",
      "681/800.0 loss: 0.23270270068886692 \n",
      "Epoch:  1\n",
      "682/800.0 loss: 0.23241020192011733 \n",
      "Epoch:  1\n",
      "683/800.0 loss: 0.23212805657228183 \n",
      "Epoch:  1\n",
      "684/800.0 loss: 0.23182749877935344 \n",
      "Epoch:  1\n",
      "685/800.0 loss: 0.23151966669588586 \n",
      "Epoch:  1\n",
      "686/800.0 loss: 0.23123895375022807 \n",
      "Epoch:  1\n",
      "687/800.0 loss: 0.23095061010843532 \n",
      "Epoch:  1\n",
      "688/800.0 loss: 0.23067271625906965 \n",
      "Epoch:  1\n",
      "689/800.0 loss: 0.23036541756566453 \n",
      "Epoch:  1\n",
      "690/800.0 loss: 0.23008623334556258 \n",
      "Epoch:  1\n",
      "691/800.0 loss: 0.22978578031472216 \n",
      "Epoch:  1\n",
      "692/800.0 loss: 0.2294851608741034 \n",
      "Epoch:  1\n",
      "693/800.0 loss: 0.22918848215631862 \n",
      "Epoch:  1\n",
      "694/800.0 loss: 0.22890752441913104 \n",
      "Epoch:  1\n",
      "695/800.0 loss: 0.22861504063930835 \n",
      "Epoch:  1\n",
      "696/800.0 loss: 0.22835668985607635 \n",
      "Epoch:  1\n",
      "697/800.0 loss: 0.22806844965722892 \n",
      "Epoch:  1\n",
      "698/800.0 loss: 0.22778175082404128 \n",
      "Epoch:  1\n",
      "699/800.0 loss: 0.2275046904518136 \n",
      "Epoch:  1\n",
      "700/800.0 loss: 0.22720525344666334 \n",
      "Epoch:  1\n",
      "701/800.0 loss: 0.22692746736979552 \n",
      "Epoch:  1\n",
      "702/800.0 loss: 0.22665658531177943 \n",
      "Epoch:  1\n",
      "703/800.0 loss: 0.22640174757477574 \n",
      "Epoch:  1\n",
      "704/800.0 loss: 0.2261115666769498 \n",
      "Epoch:  1\n",
      "705/800.0 loss: 0.22582476366332419 \n",
      "Epoch:  1\n",
      "706/800.0 loss: 0.22554701233685776 \n",
      "Epoch:  1\n",
      "707/800.0 loss: 0.22525717084964283 \n",
      "Epoch:  1\n",
      "708/800.0 loss: 0.22497364621982505 \n",
      "Epoch:  1\n",
      "709/800.0 loss: 0.22469599985215866 \n",
      "Epoch:  1\n",
      "710/800.0 loss: 0.2244094713825507 \n",
      "Epoch:  1\n",
      "711/800.0 loss: 0.2242072304538085 \n",
      "Epoch:  1\n",
      "712/800.0 loss: 0.2239294483066441 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "713/800.0 loss: 0.22365332990415207 \n",
      "Epoch:  1\n",
      "714/800.0 loss: 0.22338645973770352 \n",
      "Epoch:  1\n",
      "715/800.0 loss: 0.22314250850841902 \n",
      "Epoch:  1\n",
      "716/800.0 loss: 0.22285803377555588 \n",
      "Epoch:  1\n",
      "717/800.0 loss: 0.22258887244103098 \n",
      "Epoch:  1\n",
      "718/800.0 loss: 0.22231962392948598 \n",
      "Epoch:  1\n",
      "719/800.0 loss: 0.22204648250869166 \n",
      "Epoch:  1\n",
      "720/800.0 loss: 0.2217647583248422 \n",
      "Epoch:  1\n",
      "721/800.0 loss: 0.22150457197145 \n",
      "Epoch:  1\n",
      "722/800.0 loss: 0.2212341616849929 \n",
      "Epoch:  1\n",
      "723/800.0 loss: 0.22096639398732404 \n",
      "Epoch:  1\n",
      "724/800.0 loss: 0.22070505845135657 \n",
      "Epoch:  1\n",
      "725/800.0 loss: 0.22043047024876125 \n",
      "Epoch:  1\n",
      "726/800.0 loss: 0.22016046559271574 \n",
      "Epoch:  1\n",
      "727/800.0 loss: 0.21988920802199333 \n",
      "Epoch:  1\n",
      "728/800.0 loss: 0.21963540454163882 \n",
      "Epoch:  1\n",
      "729/800.0 loss: 0.21938590278517303 \n",
      "Epoch:  1\n",
      "730/800.0 loss: 0.2191131875484611 \n",
      "Epoch:  1\n",
      "731/800.0 loss: 0.21884980028026987 \n",
      "Epoch:  1\n",
      "732/800.0 loss: 0.2185807795597985 \n",
      "Epoch:  1\n",
      "733/800.0 loss: 0.2183314608105481 \n",
      "Epoch:  1\n",
      "734/800.0 loss: 0.2180752775296062 \n",
      "Epoch:  1\n",
      "735/800.0 loss: 0.21783321784586523 \n",
      "Epoch:  1\n",
      "736/800.0 loss: 0.21756592576911363 \n",
      "Epoch:  1\n",
      "737/800.0 loss: 0.2173058616406307 \n",
      "Epoch:  1\n",
      "738/800.0 loss: 0.21703881122101304 \n",
      "Epoch:  1\n",
      "739/800.0 loss: 0.21677076194082967 \n",
      "Epoch:  1\n",
      "740/800.0 loss: 0.21652716887152065 \n",
      "Epoch:  1\n",
      "741/800.0 loss: 0.21628340967049736 \n",
      "Epoch:  1\n",
      "742/800.0 loss: 0.21603996564620356 \n",
      "Epoch:  1\n",
      "743/800.0 loss: 0.21578181411580294 \n",
      "Epoch:  1\n",
      "744/800.0 loss: 0.21551788026754487 \n",
      "Epoch:  1\n",
      "745/800.0 loss: 0.21526966865696312 \n",
      "Epoch:  1\n",
      "746/800.0 loss: 0.21500761743879301 \n",
      "Epoch:  1\n",
      "747/800.0 loss: 0.21474461524901703 \n",
      "Epoch:  1\n",
      "748/800.0 loss: 0.21450661985733321 \n",
      "Epoch:  1\n",
      "749/800.0 loss: 0.2142570168649157 \n",
      "Epoch:  1\n",
      "750/800.0 loss: 0.21401148907016818 \n",
      "Epoch:  1\n",
      "751/800.0 loss: 0.21375584846332749 \n",
      "Epoch:  1\n",
      "752/800.0 loss: 0.21351566337108374 \n",
      "Epoch:  1\n",
      "753/800.0 loss: 0.213289609215956 \n",
      "Epoch:  1\n",
      "754/800.0 loss: 0.21302893706998288 \n",
      "Epoch:  1\n",
      "755/800.0 loss: 0.21278744154220458 \n",
      "Epoch:  1\n",
      "756/800.0 loss: 0.21254195046058735 \n",
      "Epoch:  1\n",
      "757/800.0 loss: 0.21230034139203915 \n",
      "Epoch:  1\n",
      "758/800.0 loss: 0.2120487375383674 \n",
      "Epoch:  1\n",
      "759/800.0 loss: 0.21179228393164903 \n",
      "Epoch:  1\n",
      "760/800.0 loss: 0.2115531334513982 \n",
      "Epoch:  1\n",
      "761/800.0 loss: 0.21131392713792602 \n",
      "Epoch:  1\n",
      "762/800.0 loss: 0.21108621221431195 \n",
      "Epoch:  1\n",
      "763/800.0 loss: 0.2108316936125462 \n",
      "Epoch:  1\n",
      "764/800.0 loss: 0.2105833607220572 \n",
      "Epoch:  1\n",
      "765/800.0 loss: 0.21034382110067665 \n",
      "Epoch:  1\n",
      "766/800.0 loss: 0.2100903754944596 \n",
      "Epoch:  1\n",
      "767/800.0 loss: 0.20984908374036118 \n",
      "Epoch:  1\n",
      "768/800.0 loss: 0.20961799744095155 \n",
      "Epoch:  1\n",
      "769/800.0 loss: 0.20938395208546093 \n",
      "Epoch:  1\n",
      "770/800.0 loss: 0.20915079030574296 \n",
      "Epoch:  1\n",
      "771/800.0 loss: 0.20890608930072405 \n",
      "Epoch:  1\n",
      "772/800.0 loss: 0.2086594460283877 \n",
      "Epoch:  1\n",
      "773/800.0 loss: 0.20842843747502843 \n",
      "Epoch:  1\n",
      "774/800.0 loss: 0.2081978895803613 \n",
      "Epoch:  1\n",
      "775/800.0 loss: 0.20796257177050964 \n",
      "Epoch:  1\n",
      "776/800.0 loss: 0.20773012340883873 \n",
      "Epoch:  1\n",
      "777/800.0 loss: 0.207488709835666 \n",
      "Epoch:  1\n",
      "778/800.0 loss: 0.2072471974773837 \n",
      "Epoch:  1\n",
      "779/800.0 loss: 0.20701481674630673 \n",
      "Epoch:  1\n",
      "780/800.0 loss: 0.20678061391399857 \n",
      "Epoch:  1\n",
      "781/800.0 loss: 0.20655779228033616 \n",
      "Epoch:  1\n",
      "782/800.0 loss: 0.2063216116721831 \n",
      "Epoch:  1\n",
      "783/800.0 loss: 0.20609284629237515 \n",
      "Epoch:  1\n",
      "784/800.0 loss: 0.20588435996917023 \n",
      "Epoch:  1\n",
      "785/800.0 loss: 0.20568349671433975 \n",
      "Epoch:  1\n",
      "786/800.0 loss: 0.20544381496014863 \n",
      "Epoch:  1\n",
      "787/800.0 loss: 0.20520826912045403 \n",
      "Epoch:  1\n",
      "788/800.0 loss: 0.20496923241089127 \n",
      "Epoch:  1\n",
      "789/800.0 loss: 0.20472822777714722 \n",
      "Epoch:  1\n",
      "790/800.0 loss: 0.20450570970401638 \n",
      "Epoch:  1\n",
      "791/800.0 loss: 0.2042816644099381 \n",
      "Epoch:  1\n",
      "792/800.0 loss: 0.20405723697483577 \n",
      "Epoch:  1\n",
      "793/800.0 loss: 0.20382896253735916 \n",
      "Epoch:  1\n",
      "794/800.0 loss: 0.20359547107404322 \n",
      "Epoch:  1\n",
      "795/800.0 loss: 0.2033657523857233 \n",
      "Epoch:  1\n",
      "796/800.0 loss: 0.2031546689703163 \n",
      "Epoch:  1\n",
      "797/800.0 loss: 0.20293142532061478 \n",
      "Epoch:  1\n",
      "798/800.0 loss: 0.20269481708366746 \n",
      "Epoch:  1\n",
      "799/800.0 loss: 0.2024652223382145 \n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "EPOCHS = 1\n",
    "bert_clf = BertBinaryClassifier()\n",
    "optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)\n",
    "for epoch_num in range(EPOCHS):\n",
    "    bert_clf.train()\n",
    "    train_loss = 0\n",
    "    for step_num, batch_data in enumerate(train_dataloader):\n",
    "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "        probas = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        batch_loss = loss_func(probas, labels)\n",
    "        train_loss += batch_loss.item()\n",
    "        bert_clf.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch: ', epoch_num + 1)\n",
    "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|███▏                       | 48391168/407873900 [02:36<21:42, 276081.00B/s]"
     ]
    }
   ],
   "source": [
    "bert_clf.eval()\n",
    "bert_predicted = []\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(test_dataloader):\n",
    "        token_ids, masks, labels = tuple(t for t in batch_data)\n",
    "        logits = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss() #binomial cross entropy loss\n",
    "        loss = loss_func(logits, labels) \n",
    "        numpy_logits = logits.cpu().detach().numpy()\n",
    "        \n",
    "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
    "        all_logits += list(numpy_logits[:, 0])\n",
    "        \n",
    "print(classification_report(test_y, bert_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRECISION = TRUE POS / TRUE POS + FALSE POS\n",
    "RECALL = TRUE POS / TRUE POS + FALSE NEG\n",
    "F1 = (2 * RECALL * PRECISION) / ( RECALL + PRECISION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
